{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import copy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_fraud = pd.read_csv(\"creditcard.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_fraud.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_fraud['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x281db2a3be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'Class', data = cc_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_fraud.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cc_fraud.iloc[:, 1:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cc_fraud.loc[:, 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85293    15]\n",
      " [   57    78]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9991573329588147"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first thing to note is how high the accuracy score is. This seems good, but this is a frequent problem with imbalanced datasets on display. It is easy to get a high score when there is so little of one class. If the model predicts 100 percent of classes as being 0 when only 99 percent are, there is a high accuracy but the model does nothing to inform us when the minoirty class is present. Often times with imbalanced classes we are more concerned with the minority class than the majority, so that model would not be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with this problem there are numerous alternative metrics:\n",
    "\n",
    "Precision is equal TP / TP + FP, where TP are the true positive cases and FP is the false positive cases. Put another way, it is the number of cases that are correctly identified as positive instances over the total number of cases that are predicted to be positive. \n",
    "\n",
    "Recall is equal to TP / TP + FN, where FN are the false negative cases. Put another way, recall is the number of cases that are correctly identified as positive over the total number of positive cases.\n",
    "\n",
    "There is often a tradeoff between precision and recall. One might decide to increase or decrease the threshold for which the model predicts a positive case. If the model moves from .5 to .6, for example, then the model will predict fewer positive cases and thus precision will increase as FP decreases. This will have the added effect of decreasing recall the false negatives will increase as cases that are actually positive will be identified as negative.\n",
    "\n",
    "The F1 Score is 2((precision*recall) / (precision / recall)). It seeks to balance the precision and recall in one metric.\n",
    "\n",
    "I can access these metrics with classification_report from sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     85308\\n           1       0.84      0.58      0.68       135\\n\\n    accuracy                           1.00     85443\\n   macro avg       0.92      0.79      0.84     85443\\nweighted avg       1.00      1.00      1.00     85443\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, precision is alright, while the recall is very poor at .58. Recall measures the preidicted positive cases over all positive cases. I would say with credit card fraud the effect of predicting a false negative is worse than predicting a false positive, so recall is probably the best metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRecall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code tracks the precision and recall as the threshold is altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = log.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "plt.plot(thresholds, precision[: -1], \"y--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recall[: -1], \"c--\", label=\"Recall\")\n",
    "plt.ylabel(\"Precision, Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [.01, .03, .05, .1, .15, .2, .3, .4, .5, .6, .7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.where(log.predict_proba(X_test)[:,1] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481481481481481"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame()\n",
    "for i in thresholds:\n",
    "    pred = np.where(log.predict_proba(X_test)[:,1] > i, 1, 0)\n",
    "    t = pd.DataFrame(data = {\"threshold\": [i], \"accuracy\": [accuracy_score(y_test, pred)], \"recall\": [recall_score(y_test, pred)],\n",
    "                   \"precision\":[precision_score(y_test, pred)]})\n",
    "    p = p.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.997577</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.378378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.658385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.736111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.999263</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>0.776923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.999204</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.999169</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.775862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.999169</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.801887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.585185</td>\n",
       "      <td>0.831579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.838710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.999146</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.869048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy    recall  precision\n",
       "0       0.01  0.997577  0.829630   0.378378\n",
       "0       0.03  0.999017  0.785185   0.658385\n",
       "0       0.05  0.999216  0.785185   0.736111\n",
       "0       0.10  0.999263  0.748148   0.776923\n",
       "0       0.15  0.999204  0.703704   0.772358\n",
       "0       0.20  0.999169  0.666667   0.775862\n",
       "0       0.30  0.999169  0.629630   0.801887\n",
       "0       0.40  0.999157  0.585185   0.831579\n",
       "0       0.50  0.999157  0.577778   0.838710\n",
       "0       0.60  0.999157  0.555556   0.862069\n",
       "0       0.70  0.999146  0.540741   0.869048"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned there is a clear tradeoff between recall and precision. A threshold that would lead to a relatively high score on both metrics would be around ..04. Perhaps resampling can improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near-Miss Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main ways in which to deal with imbalanced datasets. The first is through undersampling, and the other is oversampling. I will look at undersampling first. Undersampling involves selective filtering out certain values in the class which has the majority of instances. I will use the lmblearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing I had to conda install by going to the anaconda prompt: conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first kind of undersampling is the near-miss technique. This technique involves only keeping the majority class samples based off their distance from some data points of the minority class. It is similar to the K nearest neighbour technique used in classification problems. In this algorithm, data points are assigned a class based off the classes of the nearest data points. \n",
    "\n",
    "There are three different types: Near-miss 1, Near-miss 2 and Near-miss 3. The Near-miss 1 algorithm only keeps the points from the majority class that are closest to the average of a given number of the closest minority class points. \n",
    "\n",
    "The Near-miss 2 model keeps the points from the majority class that are farthest frm the average of a certain number of minority class points. \n",
    "\n",
    "The Near-miss 3 model first finds the k nearest neighbours of each instance of a minority class. It keeps the majority class points that have the smallest average distance to those nearest neighbours. \n",
    "\n",
    "For near miss 1 and 2 the number of minority class points for which the average is calculated is a hyperparameter, which can be tuned. \n",
    "\n",
    "I will use the imblearn library for resampling techniques. The first argument for under_sampling. NearMiss is sampling strategy. This argument specifies either the ratio of desired samples of the miniority class over the number of desired samples of the majority class or the target class to be resampled. This can be the majority class, all samples but the minority class and all samples but majority. The version argument specifies whether it is 1, 2 or 3. Finally, the n_neighbors argument specifies, in the case of nearmiss 3, the number of nearest neighbours for minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "nm = NearMiss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "I want to split dataset into two parts. Then, I undersample with train. After I will see how it does on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to iterate over all possible models for near miss, while varying the number of neighbours to average when finding the farthest away minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set for model 1 neighbours 1:  714\n",
      "size of training set for model 1 neighbours 2:  714\n",
      "size of training set for model 1 neighbours 3:  714\n",
      "size of training set for model 1 neighbours 4:  714\n",
      "size of training set for model 1 neighbours 5:  714\n",
      "size of training set for model 2 neighbours 1:  714\n",
      "size of training set for model 2 neighbours 2:  714\n",
      "size of training set for model 2 neighbours 3:  714\n",
      "size of training set for model 2 neighbours 4:  714\n",
      "size of training set for model 2 neighbours 5:  714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pruss\\anaconda3\\envs\\pandas_playground\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:174: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set for model 3 neighbours 1:  387\n",
      "size of training set for model 3 neighbours 2:  60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pruss\\anaconda3\\envs\\pandas_playground\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:174: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set for model 3 neighbours 3:  47\n",
      "size of training set for model 3 neighbours 4:  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pruss\\anaconda3\\envs\\pandas_playground\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:174: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set for model 3 neighbours 5:  33\n"
     ]
    }
   ],
   "source": [
    "nearmiss = pd.DataFrame(columns = [\"Model\", \"n_neighbors\", \"TP\", \"TN\", \"FP\", \"FN\", \"Precision\", \"Recall\"])\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 6):\n",
    "        nm = NearMiss(version = i, n_neighbors = j)\n",
    "        X_train, y_train = nm.fit_resample(X_train, y_train)\n",
    "        print(\"size of training set for model \" + str(i) + \" neighbours \" + str(j) + \":  \" + str(len(X_train)))\n",
    "        log = LogisticRegression()\n",
    "        log.fit(X_train, y_train)\n",
    "        y_pred = log.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        model = pd.DataFrame(data = {\"Model\": [\"NearMiss\" + str(i)], \"n_neighbors\": [str(j)]})\n",
    "        metrics = pd.DataFrame(data = {\"TP\": [cm[0][0]], \"TN\": [cm[1][1]], \"FP\": [cm[0][1]], \"FN\": [cm[1][0]]})\n",
    "        prec_rec = pd.DataFrame(data = {\"Precision\": [precision_score(y_test, y_pred)], \"Recall\": [recall_score(y_test, y_pred)], \"f1Score\": [f1_score(y_test, y_pred)]})\n",
    "        undersamples = pd.concat([model, metrics, prec_rec], axis = 1)\n",
    "        nearmiss = nearmiss.append(undersamples, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the training set decreases with nearmiss3. The nearest neighbours approach reduces the available minority samples. This has the effect of greatly reducing the recall. The precision is very low for nearmiss for all 3 models. The recall is very high in models 1 and 2, but again precision is very low. As well, the f1score, which seeks to balance the two scores is very low. In sum there does not appear to be a successful application of the nearmiss algorithm. This is likely due to the low number of minority samples. I'll try one more undersampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1Score</th>\n",
       "      <th>positive instances</th>\n",
       "      <th>negative instances</th>\n",
       "      <th>total instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NearMiss1</td>\n",
       "      <td>1</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NearMiss1</td>\n",
       "      <td>2</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NearMiss1</td>\n",
       "      <td>3</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NearMiss1</td>\n",
       "      <td>4</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NearMiss1</td>\n",
       "      <td>5</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NearMiss2</td>\n",
       "      <td>1</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NearMiss2</td>\n",
       "      <td>2</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NearMiss2</td>\n",
       "      <td>3</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NearMiss2</td>\n",
       "      <td>4</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NearMiss2</td>\n",
       "      <td>5</td>\n",
       "      <td>58029</td>\n",
       "      <td>127</td>\n",
       "      <td>27279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>58037</td>\n",
       "      <td>27406</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NearMiss3</td>\n",
       "      <td>1</td>\n",
       "      <td>64913</td>\n",
       "      <td>81</td>\n",
       "      <td>20395</td>\n",
       "      <td>54</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>64967</td>\n",
       "      <td>20476</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NearMiss3</td>\n",
       "      <td>2</td>\n",
       "      <td>51945</td>\n",
       "      <td>126</td>\n",
       "      <td>33363</td>\n",
       "      <td>9</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>51954</td>\n",
       "      <td>33489</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NearMiss3</td>\n",
       "      <td>3</td>\n",
       "      <td>55774</td>\n",
       "      <td>90</td>\n",
       "      <td>29534</td>\n",
       "      <td>45</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.006049</td>\n",
       "      <td>55819</td>\n",
       "      <td>29624</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NearMiss3</td>\n",
       "      <td>4</td>\n",
       "      <td>29514</td>\n",
       "      <td>104</td>\n",
       "      <td>55794</td>\n",
       "      <td>31</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>29545</td>\n",
       "      <td>55898</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NearMiss3</td>\n",
       "      <td>5</td>\n",
       "      <td>29550</td>\n",
       "      <td>104</td>\n",
       "      <td>55758</td>\n",
       "      <td>31</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>29581</td>\n",
       "      <td>55862</td>\n",
       "      <td>85443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model n_neighbors     TP   TN     FP  FN  Precision    Recall  \\\n",
       "0   NearMiss1           1  58029  127  27279   8   0.004634  0.940741   \n",
       "1   NearMiss1           2  58029  127  27279   8   0.004634  0.940741   \n",
       "2   NearMiss1           3  58029  127  27279   8   0.004634  0.940741   \n",
       "3   NearMiss1           4  58029  127  27279   8   0.004634  0.940741   \n",
       "4   NearMiss1           5  58029  127  27279   8   0.004634  0.940741   \n",
       "5   NearMiss2           1  58029  127  27279   8   0.004634  0.940741   \n",
       "6   NearMiss2           2  58029  127  27279   8   0.004634  0.940741   \n",
       "7   NearMiss2           3  58029  127  27279   8   0.004634  0.940741   \n",
       "8   NearMiss2           4  58029  127  27279   8   0.004634  0.940741   \n",
       "9   NearMiss2           5  58029  127  27279   8   0.004634  0.940741   \n",
       "10  NearMiss3           1  64913   81  20395  54   0.003956  0.600000   \n",
       "11  NearMiss3           2  51945  126  33363   9   0.003762  0.933333   \n",
       "12  NearMiss3           3  55774   90  29534  45   0.003038  0.666667   \n",
       "13  NearMiss3           4  29514  104  55794  31   0.001861  0.770370   \n",
       "14  NearMiss3           5  29550  104  55758  31   0.001862  0.770370   \n",
       "\n",
       "     f1Score positive instances negative instances total instances  \n",
       "0   0.009223              58037              27406           85443  \n",
       "1   0.009223              58037              27406           85443  \n",
       "2   0.009223              58037              27406           85443  \n",
       "3   0.009223              58037              27406           85443  \n",
       "4   0.009223              58037              27406           85443  \n",
       "5   0.009223              58037              27406           85443  \n",
       "6   0.009223              58037              27406           85443  \n",
       "7   0.009223              58037              27406           85443  \n",
       "8   0.009223              58037              27406           85443  \n",
       "9   0.009223              58037              27406           85443  \n",
       "10  0.007860              64967              20476           85443  \n",
       "11  0.007495              51954              33489           85443  \n",
       "12  0.006049              55819              29624           85443  \n",
       "13  0.003712              29545              55898           85443  \n",
       "14  0.003714              29581              55862           85443  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearmiss['positive instances'] = nearmiss['TP'] + nearmiss['FN']\n",
    "nearmiss['negative instances'] = nearmiss['TN'] + nearmiss['FP']\n",
    "nearmiss['total instances'] = nearmiss['positive instances'] + nearmiss['negative instances']\n",
    "nearmiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr12 = pd.concat([nearmiss.iloc[[0], :], nearmiss.iloc[[7], :]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearmissRecall = nearmiss.iloc[0, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407407407407408"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearmissRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tomek's link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tomek's link, the algorithm seeks out the data points that have the other class as a nearest neighbour. It then eliminates points based off their class. For instance, it will remove either the majority class or both instances of the classes. The aim of this algorithm is to more clearly demarcate the division between classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "FROM\n",
      "THE\n",
      "OTHER\n",
      "SIDE\n"
     ]
    }
   ],
   "source": [
    "tl = TomekLinks()\n",
    "X_train, y_train = tl.fit_resample(X_train, y_train)\n",
    "log = LogisticRegression(random_state = 3)\n",
    "log.fit(X_train, y_train)\n",
    "y_pred = log.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85293,    15],\n",
       "       [   56,    79]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5851851851851851\n",
      "0.8404255319148937\n",
      "0.6899563318777293\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(y_test, y_pred))\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tomek  = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the undersampling techniques seem to improve the recall at the cost of other scores. The scores are so low that the increase in recall (which may be the most important metric) probably isn't worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE involves creating synthetic points closely related to already present datapoints. Smote, which stands for Synthetic Minority Oversampling Technique, works by finding the nearest neighbour(s) of each minority class. It then takes the vector between both data points and multiplies by some number between 0 and 1. This sets the location for the new synthetic data point. Repeat this process k number of times, k being determined by how many nearest neighbours of each data point specified as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression() \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85308\n",
      "           1       0.07      0.90      0.13       135\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.94      0.56     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the undersampling methods, recall is increased and precision is decreased substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I next want to test out cross validation and grid search. With oversampling, the danger is overfitting as it involves creating points based off original data. In cross validation, I split the training set into 5 subsets. From there, I take 4 subsets and train data on those 4, and test on the last subset. I do this 4 times and have each subset take its turn as the validation set. I also test each training set on the test set to see if the validation scores correlate with test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "test_recall = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRecall = 0\n",
    "for train_index_ls, validation_index_ls in kf.split(X_train, y_train):\n",
    "    # keeping validation set apart and oversampling in each iteration using smote \n",
    "    train, validation = X_train[train_index_ls], X_train[validation_index_ls]\n",
    "    target_train, target_val = y_train.iloc[train_index_ls], y_train.iloc[validation_index_ls]\n",
    "    sm = SMOTE(random_state= 2)\n",
    "    X_train_res, y_train_res = sm.fit_sample(train, target_train)\n",
    "    log = LogisticRegression(random_state=2)\n",
    "    log.fit(X_train_res, y_train_res)\n",
    "    validation_preds = log.predict(validation)\n",
    "    if(recall_score(target_val, validation_preds) > bestRecall):\n",
    "        bestRecall = recall_score(target_val, validation_preds)\n",
    "        bestLog = log\n",
    "    recall.append(recall_score(target_val, validation_preds))\n",
    "    precision.append(precision_score(target_val, validation_preds))\n",
    "    accuracy.append(accuracy_score(target_val, validation_preds))\n",
    "    test_recall.append(recall_score(y_test, log.predict(X_test)))\n",
    "    f1.append(f1_score(target_val, validation_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242424242424242"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8888888888888888,\n",
       " 0.8962962962962963,\n",
       " 0.8888888888888888,\n",
       " 0.9111111111111111,\n",
       " 0.8740740740740741]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestLog.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85308\n",
      "           1       0.07      0.87      0.12       135\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.93      0.56     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9178082191780822,\n",
       " 0.8472222222222222,\n",
       " 0.9230769230769231,\n",
       " 0.9117647058823529,\n",
       " 0.9242424242424242]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmoteCV = np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmoteTest = np.mean(test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918518518518518"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmoteTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9048228989204009"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmoteCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmoteTestHigh = max(test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmoteTestHigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is very little difference in the cross validation scores and the test scores based off the models from cross validation. This is good as it shows there is no overfitting occurring. The interesting part is that the model with the highest validation score is the model with the worst test score, although it is marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous hyperparameters found in different algorithms used to predict outcomes, whether the model is found by random forest or logistic regression etc. In the case of logisitic regression, regularization is a hyperparameter. Grid search is used to check across a series of possible values for these hyperparameters and find the highest validation score from those combinations.\n",
    "\n",
    "In regularization, there is a penalty term that reduces the importance of certain parameters. The degree to which their importance is reduced is determined by a loss function. The parameters are altered so that the loss function is minimized. The motivation behind regularization is dealing with overfitting. If a model is too well-suited for a set of data, then it may not be suitable for similar yet different data. Regularization works because it introduces a variable that is independent of the data being modeled and thus the creation of the model will be less prone to overfitting. Up to a certain point it reduces the variance substantially, while only incrementing the bias. \n",
    "\n",
    "There are two kinds of regularization: Lasso, or L1, and Ridge, or L2. The main practical difference between the two is that lasso can reduce features to a weight of 0, while ridge does not. This allows Lasso to function as a type of feature selection.\n",
    "With grid search, one can test both types of regularizations and with multiple penalty terms. \n",
    "\n",
    "The higher the penalty term, the more variables will be shrunk by. When specifying the degree of penalty in Python, we use the 'C' argument, which is the inverse of the penalty term. This means a higher 'C' equates to less shrinkage and a lower C means higher shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier' , LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogCV = GridSearchCV(pipe, scoring = 'recall', param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "best_LOGcv = LogCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logGridSearch = best_LOGcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 1.623776739188721,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'liblinear'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_LOGcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6357981220657277"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logGridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model with the best recall score uses ridge regression and a C value of 1.62378. Grid search increases recall, but there clearly needs to be some resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2=LogisticRegression(C= 1.623777, solver = 'liblinear', penalty=\"l2\")\n",
    "logreg2.fit(X_train,y_train)\n",
    "y_pred = logreg2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "log2Recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5777777777777777"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with cross validation, the test recall actually goes down, despite a validation score that is higher than the original test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1) \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier' , LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_grid = GridSearchCV(pipe, scoring = 'recall', param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.5min finished\n"
     ]
    }
   ],
   "source": [
    "best_smote = smote_grid.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_grid_search = best_smote.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212992423347055"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 4.281332398719396,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'liblinear'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_smote.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSmote=LogisticRegression(C= 4.281332, solver = 'liblinear', penalty=\"l2\")\n",
    "logreg2.fit(X_train_res,y_train_res)\n",
    "y_pred = logreg2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "logSmoteRecall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9037037037037037"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logSmoteRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tomek = 0.5851851851851851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\"unchanged Logistic\": [logRecall], \"nearMiss Recall\": [nearmissRecall], \"Tomek Link\": [Tomek], \"SMOTE with no cross validation\": [smote], \"SMOTE with cross validation\": [SmoteCV], \"SMOTE Test Score\": [SmoteTestHigh], \"Grid Search with Log, Cross-validation, no resampling\": [logGridSearch], \"SMOTE_grid_search\": [smote_grid_search]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scoring = pd.DataFrame.from_dict(model_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nearMiss Recall</th>\n",
       "      <td>0.940741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE_grid_search</th>\n",
       "      <td>0.921299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Test Score</th>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE with cross validation</th>\n",
       "      <td>0.904823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE with no cross validation</th>\n",
       "      <td>0.903704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grid Search with Log, Cross-validation, no resampling</th>\n",
       "      <td>0.635798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tomek Link</th>\n",
       "      <td>0.585185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unchanged Logistic</th>\n",
       "      <td>0.577778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    recall score\n",
       "nearMiss Recall                                         0.940741\n",
       "SMOTE_grid_search                                       0.921299\n",
       "SMOTE Test Score                                        0.911111\n",
       "SMOTE with cross validation                             0.904823\n",
       "SMOTE with no cross validation                          0.903704\n",
       "Grid Search with Log, Cross-validation, no resa...      0.635798\n",
       "Tomek Link                                              0.585185\n",
       "unchanged Logistic                                      0.577778"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scoring.rename(columns = {0: \"recall score\"}).sort_values(by = 'recall score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So nearmiss recall is highest which supports undersampling, however, the other undersampling method had the worst recall other than the original data set. Tomek link barely changed any metrics from the original dataset. The grid search with no resampling had a higher cross validation score than the original dataset's test score. However, when using these optimized hyperparamters the test score was left unchanged. the mean score of cross validation indicated marginal improvement over no cross validation, as did grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
